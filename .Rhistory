clint_win.labels <- as_tibble(clinton_win=full_dataset$clinton_win);
clint_win.labels <- tibble(clinton_win=full_dataset$clinton_win);
pca.scores.clinton.win <- bind_cols(clint_win.labels, pca.scores)
ggplot(pca.scores, aes(x=Comp.1, y=Comp.2)) +
geom_point()
ggplot(pca.scores, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
geom_point()
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
geom_point()
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
geom_point()
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2")
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2") +
theme_classic() +
theme(plot.title=element_text(hjust=0.5))
pca.cor$scores
pca.cor$scores[[2]]
pca.cor$scores[["scores"]][,2]
pca.cor$scores[["scores"]][2]
pca.cor$scores[["scores"]]["Comp.2"]
pca.cor[["scores"]]["Comp.2"]
pca.cor[["scores"]][,2]
which(pca.cor[["scores"]][,2] < -20)
pca.cor[["scores"]][which(pca.cor[["scores"]][,2] < -20),2]
y.score <- pca.cor[["scores"]][which(pca.cor[["scores"]][,2] < -20),2]
x.score <- pca.cor[["scores"]][which(pca.cor[["scores"]][,2] < -20),1]
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
annotate(geom = "point", x=x.score, y=y.score, size=2, color="orange") +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2") +
theme_classic() +
theme(plot.title=element_text(hjust=0.5))
pca.scores <- as_tibble(pca.cor[["scores"]]);
clint_win.labels <- tibble(clinton_win=full_dataset$clinton_win);
pca.scores.clinton.win <- bind_cols(clint_win.labels, pca.scores)
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
annotate(geom = "point", x=x.score, y=y.score, size=3, color="orange") +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2") +
theme_classic() +
theme(plot.title=element_text(hjust=0.5))
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
annotate(geom = "point", x=x.score, y=y.score, size=5, color="orange") +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2") +
theme_classic() +
theme(plot.title=element_text(hjust=0.5))
ggplot(pca.scores.clinton.win, aes(x=Comp.1, y=Comp.2, color=clinton_win)) +
scale_color_manual(values=c("red", "blue")) +
annotate(geom = "point", x=x.score, y=y.score, size=5, color="green") +
geom_point() +
labs(x="Component 1", y="Component 2",
title="Score Plot for Components 1 and 2") +
theme_classic() +
theme(plot.title=element_text(hjust=0.5))
full_dataset[which(pca.cor[["scores"]][,2] < -20),"county_name"]
full_dataset[which(pca.cor[["scores"]][,2] < -20),"state"]
pca.cor$loadings
### --- NN with Keras --- ###
### -- Prepare the data for the NN --- ###
labels <- fastDummies::dummy_cols(labels, remove_first_dummy = T);
dataset.NN <- bind_cols(predictors.full.dataset, labels[,"clinton_win_TRUE"])
train.indices <- createDataPartition(dataset.NN$clinton_win_TRUE, p=0.7, list=F);
train.set <- dataset.NN[train.indices,];
test.set <- dataset.NN[-train.indices,];
View(train.set)
### --- NN with Keras --- ###
### -- Prepare the data for the NN --- ###
labels <- fastDummies::dummy_cols(labels, remove_first_dummy = T);
labels <- full_dataset %>% select(clinton_win)
pca.cor <- princomp(predictors.full.dataset, cor=T);
### --- NN with Keras --- ###
### -- Prepare the data for the NN --- ###
labels <- fastDummies::dummy_cols(labels, remove_first_dummy = T);
dataset.NN <- bind_cols(predictors.full.dataset, labels[,"clinton_win_TRUE"])
train.indices <- createDataPartition(dataset.NN$clinton_win_TRUE, p=0.7, list=F);
train.set <- dataset.NN[train.indices,];
test.set <- dataset.NN[-train.indices,];
train.set.predictors <- train.set %>% select(-clinton_win_TRUE) %>% scale();
labels.train <- keras::to_categorical(train.set$clinton_win_TRUE);
test.set.predictors <- test.set %>% select(-clinton_win_TRUE) %>% scale();
labels.test <-to_categorical(test.set$clinton_win_TRUE);
dnn.model <- keras_model_sequential();
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='sigmoid')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model %>% evaluate(test.set.predictors, labels.test)
predictions <- dnn.model %>% predict_classes(test.set.predictors);
table(factor(predictions, levels=min(test.set$clinton_win_TRUE):max(test.set$clinton_win_TRUE)),
factor((test.set$clinton_win_TRUE), levels=min(test.set$clinton_win_TRUE):max(test.set$clinton_win_TRUE)))
labels <- full_dataset %>% select(clinton_win)
### --- NN with Keras --- ###
### -- Prepare the data for the NN --- ###
labels <- fastDummies::dummy_cols(labels, remove_first_dummy = T);
View(labels)
nn.test.acc <- dnn.model$acc
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
nn.test.acc <- dnn.model$acc
dnn.model.mets <- dnn.model$acc
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
dnn.model.acc <- dnn.model.mets$acc
### --- KNN model --- ###
knn.mod <- class::knn(train=train.set, test=test.set, cl=train.set$clinton_win_TRUE, k=50);
round(mean(knn.mod != test.set$clinton_win_TRUE) * 100, digits=2)
knn.acc <- round(mean(knn.mod != test.set$clinton_win_TRUE) * 100, digits=2)
### --- Random Forest Model --- ###
i <- 49
set.seed(i)
train.indices.rf <- createDataPartition(full_dataset$clinton_win, p=0.8, list=F);
train.set.rf <- full_dataset[train.indices.rf,c(3:11, 17)]
test.set.rf <- full_dataset[-train.indices.rf,c(3:11, 17)]
train.set.rf$clinton_win <- factor(as.character(train.set.rf$clinton_win));
test.set.rf$clinton_win <- factor(as.character(test.set.rf$clinton_win));
contrasts(test.set.rf$clinton_win);
contrasts(train.set.rf$clinton_win);
set.seed(i)
rf_clinton <- randomForest::randomForest(clinton_win ~ ., data=train.set.rf, ntree=150,
mtry=1, importance=T);
rf_clinton
preds <- predict(rf_clinton, test.set.rf[,-10])
conf.matrix <- table(observed=test.set.rf$clinton_win, predicted=preds)
accuracy <- sum(diag(conf.matrix)) / sum(conf.matrix)
error.rate <- 1 - accuracy;
preds.roc <- predict(rf_clinton, newdata=test.set.rf[,-10], type="prob")
forest.pred <- prediction(preds.roc[,2], test.set.rf$clinton_win);
forest.perf <- performance(forest.pred, "tpr", "fpr");
plot(forest.perf, main="ROC", colorize=F);
classes <- levels(test.set.rf$clinton_win)
true.values <- ifelse(test.set.rf[,10] == classes[1], 1, 0);
pred.auc <- prediction(preds.roc[,1], true.values)
auc <- performance(pred.auc, measure="auc")
auc@y.values
accuracy
plot(forest.perf, main="ROC", colorize=F);
lines(x=seq(0, 1, by=0.01), y=seq(0, 1, by=0.01), lty=2, col="red")
accuracy.rf <- sum(diag(conf.matrix)) / sum(conf.matrix)
error.rate.rf <- 1 - accuracy;
knn.acc <- round(mean(knn.mod != test.set$clinton_win_TRUE), digits=2)
knn.err <- 1 - knn.acc;
tibble(acc=c(classifier=c("DNN", "KNN", "Random Forest"),
dnn.model.acc, knn.acc, accuracy.rf))
tibble(classifier=c("DNN", "KNN", "Random Forest"),
acc=c(dnn.model.acc, knn.acc, accuracy.rf))
acc.tibble <- tibble(classifier=c("DNN", "KNN", "Random Forest"),
acc=c(dnn.model.acc, knn.acc, accuracy.rf))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity")
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=format(acc, digits=2, nsmall=2)))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=format(acc, digits=2, nsmall=2),
position=position_dodge(0.9)))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=format(acc, digits=2, nsmall=2),
vjust=-1.0))
acc.tibble <- tibble(classifier=c("DNN", "KNN", "Random Forest"),
acc=c(dnn.model.acc * 100, knn.acc * 100, accuracy.rf * 100))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=format(acc, digits=2, nsmall=2),
vjust=-1.0))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0))
acc.tibble <- tibble(classifier=c("DNN", "KNN", "Random Forest"),
acc=c(100 - dnn.model.acc * 100, 100 - knn.acc * 100, 100 - accuracy.rf * 100))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy ")
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity", fill="orange") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy ") +
theme_minimal() +
theme(plot.title=element_text(hjust=0.5))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity", fill="orange") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy vs. Classifier Type") +
theme_minimal() +
theme(plot.title=element_text(hjust=0.5))
acc.tibble <- tibble(classifier=c("Dense DL Neural Network", "KNN", "Random Forest"),
acc=c(100 - dnn.model.acc * 100, 100 - knn.acc * 100, 100 - accuracy.rf * 100))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity", fill="orange") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy vs. Classifier Type") +
theme_minimal() +
theme(plot.title=element_text(hjust=0.5))
acc.tibble <- tibble(classifier=c("Deep-Learning Neural Network", "KNN", "Random Forest"),
acc=c(100 - dnn.model.acc * 100, 100 - knn.acc * 100, 100 - accuracy.rf * 100))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity", fill="orange") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy vs. Classifier Type") +
theme_minimal() +
theme(plot.title=element_text(hjust=0.5))
dnn.model <- keras_model_sequential();
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='softmax')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
dnn.model.acc <- dnn.model.mets$acc
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='sigmoid')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
dnn.model.acc <- dnn.model.mets$acc
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='tanh')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model <- keras_model_sequential();
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='tanh')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
dnn.model.acc <- dnn.model.mets$acc
dnn.model <- keras_model_sequential();
dnn.model %>%
layer_dense(units=32, activation="relu", input_shape=ncol(train.set.predictors)) %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=128, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=64, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=32, activation = "relu") %>%
layer_dropout(rate=0.1) %>%
layer_dense(units=2, activation='sigmoid')
history <- dnn.model %>% compile(
loss = "binary_crossentropy",
optimizer = "RMSprop",
metrics = c("accuracy")
);
dnn.model %>% fit(
train.set.predictors, labels.train,
epochs = 100,
batch_size = 5,
validation_split=0.2
)
dnn.model.mets <- dnn.model %>% evaluate(test.set.predictors, labels.test)
dnn.model.acc <- dnn.model.mets$acc
predictions <- dnn.model %>% predict_classes(test.set.predictors);
table(factor(predictions, levels=min(test.set$clinton_win_TRUE):max(test.set$clinton_win_TRUE)),
factor((test.set$clinton_win_TRUE), levels=min(test.set$clinton_win_TRUE):max(test.set$clinton_win_TRUE)))
acc.tibble <- tibble(classifier=c("Deep-Learning Neural Network", "KNN", "Random Forest"),
acc=c(100 - dnn.model.acc * 100, 100 - knn.acc * 100, 100 - accuracy.rf * 100))
ggplot(acc.tibble, aes(x=classifier, y=acc)) +
geom_bar(stat="identity", fill="orange") +
geom_text(aes(label=paste(format(acc, digits=2, nsmall=2), "%", sep=""),
vjust=-1.0)) +
labs(x=NULL, y="Accuracy (%)", title="Model Accuracy vs. Classifier Type") +
theme_minimal() +
theme(plot.title=element_text(hjust=0.5))
### --- KNN model --- ###
knn.mod <- class::knn(train=train.set, test=test.set, cl=train.set$clinton_win_TRUE, k=3);
knn.acc <- round(mean(knn.mod != test.set$clinton_win_TRUE), digits=2)
knn.err <- 1 - knn.acc;
### --- KNN model --- ###
knn.mod <- class::knn(train=train.set, test=test.set, cl=train.set$clinton_win_TRUE, k=5);
knn.acc <- round(mean(knn.mod != test.set$clinton_win_TRUE), digits=2)
knn.err <- 1 - knn.acc;
### --- KNN model --- ###
knn.mod <- class::knn(train=train.set, test=test.set, cl=train.set$clinton_win_TRUE, k=5);
knn.acc <- round(mean(knn.mod != test.set$clinton_win_TRUE), digits=2)
knn.err <- 1 - knn.acc;
man.full.data <- manova(cbind(median_age, mean_savings, per_capita_income, percent_in_poverty, percent_veterans,
percent_female, population_density, percent_in_nursing_homes, crime_index_PC) ~ as.factor(clinton_win),
data=full_dataset)
res.man <- as_tibble(residuals(man.full.data))
obs.tibble <- tibble(observation_num=1:nrow(res.man))
residuals.obs <- dplyr::bind_cols(obs.tibble, res.man)
summary(man.full.data, test="Wilks")
summary.aov(man.full.data)
for (i in 1:p) {
lower[i] <- full.dataset.means[i] - sqrt(crit.value) * sqrt(vars.full.dataset[i] / n);
upper[i] <- full.dataset.means[i] + sqrt(crit.value) * sqrt(vars.full.dataset[i] / n);
print(paste("The confidence interval for ", variable_names_format[i], "is:"))
cat(paste("\t(", format(lower[i], digits=2, nsmall=2, big.mark=","), ", " , format(upper[i], digits=2, nsmall=2, big.mark=","), ")\n\n", sep=""))
}
qq1 <- ggplot(full_dataset, aes(sample=sqrt(crime_index_PC), color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", sqrt("Crime Index Per Capita"), sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq1 <- ggplot(full_dataset, aes(sample=crime_index_PC, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", sqrt("Crime Index Per Capita"), sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq1
qq1 <- ggplot(full_dataset, aes(sample=mean_savings, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", sqrt("Crime Index Per Capita"), sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq1
ggplot(full_dataset, aes(x=mean_savings)) +
geom_histogram()
ggplot(full_dataset, aes(sample=percent_female, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", sqrt("Crime Index Per Capita"), sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
ggplot(full_dataset, aes(x=percent_female)) +
geom_histogram()
qq1 <- ggplot(full_dataset, aes(sample=crime_index_PC, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Crime Index Per Capita", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq2 <- ggplot(full_dataset, aes(sample=median_age, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Median Age", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq3 <- ggplot(full_dataset, aes(sample=mean_savings, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Mean Savings", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq4 <- ggplot(full_dataset, aes(sample=per_capita_income, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Per Capita Income", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq5 <- ggplot(full_dataset, aes(sample=percent_in_poverty, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Percent In Poverty", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq6 <- ggplot(full_dataset, aes(sample=percent_veterans, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Crime Index Per Capita", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq7 <- ggplot(full_dataset, aes(sample=percent_female, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Percent Female", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq8 <- ggplot(full_dataset, aes(sample=population_density, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Population Density", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq9 <- ggplot(full_dataset, aes(sample=percent_in_nursing_homes, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", "Percent In Nursing Homes", sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
qq1 + qq2 + qq3 + qq4 + qq5 + qq6 + qq7 + qq8 + qq9 + guide_area() + plot_layout(ncol=2, guides="collect");
qq9
ggplot(full_dataset, aes(sample=population_density, color=clinton_win)) +
stat_qq() + stat_qq_line() +
scale_color_manual(values=c("red", "blue")) +
labs(x=NULL, y=NULL, title=expression(paste("QQ-Plot of ", sqrt("Crime Index Per Capita"), sep=" "))) +
theme_classic() +
theme(plot.title = element_text(hjust=0.5))
ggplot(full_dataset, aes(x=population_density)) +
geom_histogram()
plot(forest.perf, main="ROC Curve for the Random Forest", colorize=F);
lines(x=seq(0, 1, by=0.01), y=seq(0, 1, by=0.01), lty=2, col="red")
auc@y.values
conf.matrix <- table(observed=test.set.rf$clinton_win, predicted=preds)
conf.matrix
regsubsets(clinton_win ~., data=full_dataset)
install.packages("leaps")
library(leaps)
regsubsets(clinton_win ~., data=full_dataset)
install.packages("glmulti")
library(glmulti)
mylog <- glm(clinton_win ~ ., data=full_dataset, family="binomial")
